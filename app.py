import os
import subprocess
import uuid
import sys # <-- Add missing import
import platform # <-- Add for OS detection
from flask import Flask, request, jsonify, send_from_directory
from openai import OpenAI, OpenAIError # Assuming openai library is installed

app = Flask(__name__, static_folder='.', static_url_path='')

# In-memory storage for conversation history (replace with a database for production)
conversation_history = []
# Global variable to store the result of the last JMeter run
last_jmeter_result = {"status": "none", "message": "No JMeter test run yet."}


# --- Helper Functions ---

def generate_unique_filenames(base_jmx_path):
    """Generates unique names for JTL and report directory."""
    unique_id = uuid.uuid4()
    jtl_filename = f"results_{unique_id}.jtl"
    report_dir = f"report_{unique_id}"
    # Ensure the report dir is relative to the script or an absolute path
    # For simplicity, let's assume it's created in the CWD of app.py
    report_path = os.path.join(os.getcwd(), report_dir)
    return jtl_filename, report_path

def call_openai_api(api_key, user_message, jmeter_path):
    """Calls OpenAI API to get structured response."""
    global conversation_history
    client = OpenAI(api_key=api_key)

    # Basic system prompt - enhance as needed
    system_prompt = f"""You are an AI assistant helping users run JMeter tests.
The user's JMeter installation bin directory is: {jmeter_path}
When the user asks to run a JMeter test plan (.jmx file), identify the file path.
Construct the JMeter command line for non-GUI execution (-n), specifying the test plan (-t), a result file (-l), and enabling report generation (-e -o).
Generate unique names for the result file (.jtl) and the report output directory.

If the user asks to open or launch the JMeter GUI, respond with "type": "launch_jmeter_gui" and no "command".

Respond ONLY in JSON format with two keys:
1.  "user_response": A friendly, conversational string response for the user.
2.  "technical_action": An object with:
    - "type": "run_jmeter_test" and "command": "full_jmeter_cli_command" for running tests.
    - "type": "launch_jmeter_gui" for opening the GUI.
    - OR null if no technical action is needed.

Example run_jmeter_test command format: {jmeter_path}/jmeter -n -t /path/to/testplan.jmx -l unique_results.jtl -e -o /path/to/unique_report_dir
Example launch_jmeter_gui action: {{"type": "launch_jmeter_gui"}}
"""

    messages = [{"role": "system", "content": system_prompt}]
    # Add recent history (optional, manage token limits)
    messages.extend(conversation_history[-5:]) # Keep last 5 exchanges
    messages.append({"role": "user", "content": user_message})

    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo", # Or another suitable model
            messages=messages,
            response_format={ "type": "json_object" }
        )
        response_content = completion.choices[0].message.content
        # Add exchange to history
        conversation_history.append({"role": "user", "content": user_message})
        conversation_history.append({"role": "assistant", "content": response_content}) # Store the structured response
        return response_content # This should be the JSON string
    except OpenAIError as e:
        print(f"OpenAI API error: {e}")
        # Return error details in a format the frontend can potentially handle
        # Using a tuple to signal error within the chat function
        return (jsonify({"error": f"OpenAI API error: {str(e)}"}), 500)
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Using a tuple to signal error within the chat function
        return (jsonify({"error": f"Failed to process request with AI: {str(e)}"}), 500)


def run_jmeter_script(jmeter_path, command):
    """Calls the jmeter_runner.py script."""
    try:
        # Ensure the command uses the correct jmeter executable path
        # (The command generated by OpenAI should already include it based on the prompt)
        print(f"Executing JMeter Runner with command: {command}")

        # We need jmeter_runner.py to exist first.
        # For now, let's simulate the call and assume success.
        # In the next step, we'll create jmeter_runner.py and use subprocess properly.

        # Actual implementation: Call jmeter_runner.py
        # Ensure python executable is correct (e.g., sys.executable or just 'python')
        python_executable = sys.executable # Use the same python that runs flask
        runner_script_path = os.path.join(os.path.dirname(__file__), 'jmeter_runner.py')

        # Pass jmeter_path and the full command string as separate arguments
        process = subprocess.run(
            [python_executable, runner_script_path, jmeter_path, command],
            capture_output=True, text=True, check=False # check=False to parse output manually
        )

        print(f"jmeter_runner.py stdout:\n{process.stdout}")
        print(f"jmeter_runner.py stderr:\n{process.stderr}")

        if process.returncode == 0 and "Status: success" in process.stdout:
            # Parse output from jmeter_runner.py
            output_lines = process.stdout.strip().splitlines()
            status = "success"
            report_path = None
            for line in output_lines:
                if line.startswith("Report Path:"):
                    report_path = line.split(": ", 1)[1]
                    break
            return {"status": status, "report_path": report_path, "output": process.stdout}
        else:
            # Capture error message from runner script output or stderr
            error_message = process.stderr
            if not error_message: # If stderr is empty, check stdout for error message
                 output_lines = process.stdout.strip().splitlines()
                 for line in output_lines:
                     if line.startswith("Message:"):
                         error_message = line.split(": ", 1)[1]
                         break
            if not error_message: # Fallback if no specific message found
                 error_message = "JMeter runner script failed. Check logs."

            return {"status": "error", "message": error_message}

    except FileNotFoundError:
        # This might happen if python executable is not found, though unlikely if Flask is running
        print(f"Error: Python executable not found at '{python_executable}'")
        return {"status": "error", "message": "Python executable not found"}
    except Exception as e:
        print(f"Unexpected error running JMeter script via subprocess: {e}")
        # Attempt to get stderr if available, otherwise use general exception string
        error_message = getattr(e, 'stderr', str(e))
        return {"status": "error", "message": error_message}
    except Exception as e: # Catch broader exceptions last
        print(f"General unexpected error in run_jmeter_script: {e}")
        return {"status": "error", "message": str(e)}


def analyze_jmeter_error_with_openai(api_key, error_message):
    """Calls OpenAI API to analyze a JMeter error message."""
    # Need json import here if not already imported globally
    import json
    client = OpenAI(api_key=api_key)

    system_prompt = f"""You are an AI assistant helping users troubleshoot JMeter errors.
The user encountered the following error message while trying to run a JMeter command:
--- ERROR START ---
{error_message}
--- ERROR END ---

Please analyze this error message.
Respond ONLY in JSON format with two keys:
1.  "user_response": A friendly, conversational string explaining the likely cause of the error in simple terms and suggesting what the user might check (e.g., file paths, JMeter logs, command syntax).
2.  "technical_action": An object suggesting a concrete next step, like {{"type": "check_log", "log_path": "/path/to/jmeter.log"}} or {{"type": "review_command"}} or null if no specific action is recommended beyond the user_response explanation.
"""
    messages = [{"role": "system", "content": system_prompt}]

    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo", # Or another suitable model
            messages=messages,
            response_format={ "type": "json_object" }
        )
        response_content = completion.choices[0].message.content
        return response_content # Return the JSON string
    except OpenAIError as e:
        print(f"OpenAI API error during error analysis: {e}")
        # Return a fallback JSON indicating the analysis failed
        return json.dumps({"user_response": f"Failed to analyze the JMeter error with AI: {str(e)}", "technical_action": None})
    except Exception as e:
        print(f"Error calling OpenAI for error analysis: {e}")
        return json.dumps({"user_response": f"An unexpected error occurred while analyzing the JMeter error: {str(e)}", "technical_action": None})


# --- Validation Function ---

def validate_configuration(jmeter_path, api_key):
    """Checks JMeter path and OpenAI key."""
    results = {"jmeter_valid": False, "openai_valid": False, "jmeter_error": None, "openai_error": None}

    # 1. Validate JMeter Path
    try:
        current_os = platform.system()
        jmeter_executable_name = "jmeter.bat" if current_os == "Windows" else "jmeter"
        full_executable_path = os.path.join(jmeter_path, jmeter_executable_name)

        if not os.path.exists(full_executable_path):
            results["jmeter_error"] = f"Executable not found at: {full_executable_path}"
        elif not os.access(full_executable_path, os.X_OK):
            results["jmeter_error"] = f"No execute permission for: {full_executable_path}"
        else:
            results["jmeter_valid"] = True
    except Exception as e:
        results["jmeter_error"] = f"Error checking JMeter path: {str(e)}"

    # 2. Validate OpenAI Key (make a simple, low-cost API call)
    if api_key:
        try:
            client = OpenAI(api_key=api_key)
            # Example: List models (adjust if needed, check for low-cost options)
            client.models.list()
            results["openai_valid"] = True
        except OpenAIError as e:
            results["openai_error"] = f"OpenAI API key error: {str(e)}"
        except Exception as e:
            results["openai_error"] = f"Error validating OpenAI key: {str(e)}"
    else:
        results["openai_error"] = "OpenAI API key is missing."

    return results

# --- Flask Routes ---

@app.route('/')
def index():
    # Serve the main HTML file from the static folder
    return send_from_directory(app.static_folder, 'index.html')

@app.route('/chat', methods=['POST'])
def chat():
    global last_jmeter_result # Declare global at the start of the function
    data = request.json
    user_message = data.get('message')
    jmeter_path = data.get('jmeter_path')
    openai_key = data.get('openai_key')

    if not all([user_message, jmeter_path, openai_key]):
        return jsonify({"error": "Missing message, jmeter_path, or openai_key"}), 400

    try:
        # 1. Call OpenAI
        ai_response_or_error = call_openai_api(openai_key, user_message, jmeter_path)
        if isinstance(ai_response_or_error, tuple): # Error occurred in call_openai_api
             return ai_response_or_error # Return the (jsonify({...}), status_code) tuple

        ai_response_json_str = ai_response_or_error

        # Parse the JSON string response from OpenAI
        import json
        try:
            ai_response = json.loads(ai_response_json_str)
        except json.JSONDecodeError:
             print(f"Error decoding OpenAI JSON response: {ai_response_json_str}")
             return jsonify({"error": "Failed to decode AI response"}), 500


        user_facing_response = ai_response.get("user_response", "Sorry, I couldn't process that.")
        technical_action = ai_response.get("technical_action")

        # 2. Prepare the full response payload
        response_payload = ai_response # Send the whole parsed AI response

        # 3. Perform technical action (if any) - *Ideally in a background task*
        # This logic remains the same, it just happens after preparing the full response
        if technical_action and technical_action.get("type") == "run_jmeter_test":
            command = technical_action.get("command")
            if command:
                # global last_jmeter_result # Removed from here
                print(f"Received technical action: run_jmeter_test")
                last_jmeter_result = {"status": "running", "message": f"Executing command: {command}"} # Update status before running

                # Run the script
                run_result = run_jmeter_script(jmeter_path, command)

                # Check the result and potentially analyze error with AI
                if run_result.get("status") == "error":
                    print(f"JMeter run failed. Analyzing error: {run_result.get('message')}")
                    # Call the new function to get AI analysis of the error
                    # Ensure json is imported if analyze_jmeter_error_with_openai doesn't import it
                    import json
                    error_analysis_json_str = analyze_jmeter_error_with_openai(openai_key, run_result.get("message", "Unknown error"))
                    try:
                        # Parse the analysis
                        analysis_result = json.loads(error_analysis_json_str)
                        # Store the analysis along with status and original error
                        last_jmeter_result = {
                            "status": "error_analyzed",
                            "user_response": analysis_result.get("user_response"),
                            "technical_action": analysis_result.get("technical_action"),
                            "original_error_message": run_result.get("message", "Unknown original error") # Include original error
                        }
                        print(f"JMeter error analysis result stored: {last_jmeter_result}")
                    except json.JSONDecodeError:
                        print(f"Failed to decode error analysis JSON: {error_analysis_json_str}")
                        # Store a fallback error message, including original error if possible
                        last_jmeter_result = {
                            "status": "error_analysis_failed",
                            "user_response": "Failed to parse AI analysis of the JMeter error.",
                            "technical_action": None,
                            "original_error_message": run_result.get("message", "Unknown original error")
                        }
                else:
                    # If successful, store the original run_result
                    last_jmeter_result = run_result
                    print(f"JMeter run successful. Result stored: {last_jmeter_result}")

                # TODO: Find a way to inform the user about the background task result (e.g., WebSockets) - Note remains relevant
        elif technical_action and technical_action.get("type") == "launch_jmeter_gui":
            # global last_jmeter_result # Removed from here
            print(f"Received technical action: launch_jmeter_gui")
            try:
                current_os = platform.system()
                jmeter_executable_name = "jmeter.bat" if current_os == "Windows" else "jmeter"
                full_executable_path = os.path.join(jmeter_path, jmeter_executable_name)

                if not os.path.exists(full_executable_path):
                     raise FileNotFoundError(f"JMeter executable not found at {full_executable_path}")

                # Use Popen to start GUI in the background without waiting
                subprocess.Popen([full_executable_path], cwd=jmeter_path) # Run from JMeter bin dir
                last_jmeter_result = {"status": "gui_launched", "message": f"Attempted to launch JMeter GUI from: {full_executable_path}"}
                print(f"JMeter GUI launch attempted. Result stored: {last_jmeter_result}")

            except Exception as launch_error:
                 print(f"Error launching JMeter GUI: {launch_error}")
                 last_jmeter_result = {"status": "gui_launch_failed", "message": f"Failed to launch JMeter GUI: {str(launch_error)}"}


        return jsonify(response_payload)

    except Exception as e:
        print(f"Error in /chat endpoint: {e}")
        return jsonify({"error": "An internal server error occurred"}), 500


@app.route('/validate_config', methods=['POST'])
def validate_config_route():
    data = request.json
    jmeter_path = data.get('jmeter_path')
    openai_key = data.get('openai_key')

    if not jmeter_path or not openai_key:
        return jsonify({"error": "Missing jmeter_path or openai_key"}), 400

    validation_results = validate_configuration(jmeter_path, openai_key)
    return jsonify(validation_results)


@app.route('/get_last_jmeter_result', methods=['GET'])
def get_last_jmeter_result_route():
    # global last_jmeter_result # Removed global as we are only reading here
    return jsonify(last_jmeter_result)


if __name__ == '__main__':
    # Make sure debug=False for production
    app.run(debug=True, port=5000) # Runs on http://127.0.0.1:5000
